{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cb5f5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3c5a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'was', 'born', 'in', 'Germany', 'in', '2000.', 'Thus,', 'I', 'grew', 'up', 'in', 'Germany.']\n",
      "['I was born in Germany in 2000. Thus', ' I grew up in Germany.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I was born in Germany in 2000. Thus, I grew up in Germany.\"\n",
    "print(sentence.split())\n",
    "print(sentence.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c85333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7941bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sometime', 'me', 'therefore', 'doing', 'nevertheless', 'nobody', 'alone', 'five', 'still', 'everywhere', 'without', 'you', 'those', 'he', 'until', 'itself', 'every', 'could', 'one', 'anyway', 'can', 'not', 'several', 'whether', 'their', 'who', \"'d\", 'none', 'again', 'back', 'up', 'ever', 'another', 'namely', 'down', 'meanwhile', 'anyone', 'please', 'three', 'make', '’s', 'here', '’ll', 'first', 'wherein', 'thereupon', 'last', 'otherwise', 'such', 'being', 'but', 'four', '‘ll', 'though', 'more', 'already', 'whoever', 'for', \"'ve\", 'something', '’d', 'latterly', 'amount', 'hundred', 'nothing', 'perhaps', 'further', 'since', 'hence', 'if', 'through', 'say', 'however', 'n‘t', 'front', 'eight', 'during', 'below', 'always', 'thereafter', 'onto', 'what', 'thus', 'which', 'wherever', 'she', 'two', 'hereupon', 'quite', \"'re\", 'about', \"'ll\", 'often', 'call', 'yours', '‘m', 'never', '‘d', 'that', 'go', 'twenty', 'than', 'used', 'when', 'elsewhere', 'then', 'cannot', 'are', 'few', 'everyone', 'ca', 'on', 'thence', 'mostly', 'ours', 'been', 'from', 'much', 'become', 'noone', 'so', 'had', 'indeed', 'her', 'herself', 'moreover', 'next', 'per', 'full', 'off', 'each', 'any', 'bottom', 'third', 'give', 'was', '’m', 'there', 'formerly', 'using', 'too', 'just', 'across', 'whereby', '’re', 'also', '‘s', 'either', 'mine', 'my', 'myself', 'with', 'seem', 'anything', 'how', 'may', 'move', 'between', 'therein', 'your', 'nor', 'an', 'show', 'anyhow', 'above', 'behind', 'out', 'well', 'it', 'only', 'his', 'i', 'nowhere', 'because', 'becomes', 'within', 'ten', 'both', 'along', 'whose', 'around', 'sometimes', 'together', 'yet', 'do', 'empty', 'once', 'where', 'beside', 'take', 'might', 'themselves', 'really', 'against', 'nine', 'throughout', 'fifteen', 'has', 'after', 'thru', 'among', 'top', 'before', 'twelve', 'will', 'via', 'various', 'rather', 'name', 'yourself', 'becoming', 'does', 'serious', 'everything', 'seems', 'besides', 'himself', 'somehow', 'made', 'him', 'hereafter', 'towards', 'whatever', 'is', 'by', 're', 'side', 'became', 'and', 'our', 'whenever', 'under', 'hers', 'keep', 'this', 'now', 'six', 'thereby', 'see', 'enough', 'upon', \"'m\", 'to', 'eleven', 'afterwards', 'many', 'somewhere', 'whom', 'get', 'should', 'ourselves', 'seemed', 'some', 'latter', \"'s\", '‘re', 'fifty', 'be', 'why', 'whence', 'or', 'of', 'most', 'the', 'even', 'would', 'while', 'yourselves', 'as', 'others', 'neither', 'whither', 'toward', 'except', 'beforehand', 'least', 'them', 'put', 'n’t', 'seeming', 'own', 'less', 'herein', 'very', 'these', 'regarding', 'us', 'anywhere', 'did', 'amongst', 'beyond', 'no', 'over', 'they', 'due', 'must', 'although', 'part', 'into', 'same', 'former', 'have', 'whereafter', 'unless', 'whole', 'other', 'in', 'whereas', 'done', 'someone', 'we', 'at', 'else', 'hereby', 'whereupon', 'were', '‘ve', 'sixty', '’ve', 'almost', 'all', 'am', \"n't\", 'forty', 'its', 'a'}\n"
     ]
    }
   ],
   "source": [
    "text = \"When I first met her she was very quiet. She remained quiet during the\"\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "print(sw_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e1b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  70  -  When I first met her she was very quiet. She remained quiet during the\n",
      "New length:  25  -  met quiet. remained quiet\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "new_text = \" \".join(words)\n",
    "print(\"Old length: \", len(text),\" - \", text)\n",
    "print(\"New length: \", len(new_text),\" - \", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52114941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 10, 4, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "w=[\n",
    "[1,1,2,1,1,1,1,0,0],\n",
    "[1,1,0,2,1,0,1,0,4],\n",
    "[0,1,1,1,0,1,0,0,0],\n",
    "[2,1,2,2,0,0,1,2,2],\n",
    "[3,1,0,3,0,0,1,2,3]\n",
    "]\n",
    "total = [sum(x) for x in w]\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c24f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wines: [4, 5, 3, 5, 2, 2, 4, 2, 3]\n",
      "IDF : [ 0.22314355 -0.          0.51082562 -0.          0.91629073  0.91629073\n",
      "  0.22314355  0.91629073  0.51082562]\n",
      "TF :\n",
      " [[0.125      0.1        0.         0.16666667 0.23076923]\n",
      " [0.125      0.1        0.25       0.08333333 0.07692308]\n",
      " [0.25       0.         0.25       0.16666667 0.        ]\n",
      " [0.125      0.2        0.25       0.16666667 0.23076923]\n",
      " [0.125      0.1        0.         0.         0.        ]\n",
      " [0.125      0.         0.25       0.         0.        ]\n",
      " [0.125      0.1        0.         0.08333333 0.07692308]\n",
      " [0.         0.         0.         0.16666667 0.15384615]\n",
      " [0.         0.4        0.         0.16666667 0.23076923]]\n",
      "Total : [1.0, 1.0, 1.0, 0.9999999999999999, 1.0]\n"
     ]
    }
   ],
   "source": [
    "w_t = np.asarray(w).transpose()\n",
    "wines = [ np.count_nonzero(x) for x in w_t]\n",
    "IDF = (-1)*np.log(np.divide(np.asarray(wines),5))\n",
    "print(\"wines: \" + str(wines))\n",
    "print(\"IDF : \" + str(IDF))\n",
    "w_tf = np.divide(w_t, total)\n",
    "total2 = [sum(x) for x in w_tf.transpose()]\n",
    "print(\"TF :\\n \"+ str(w_tf))\n",
    "print(\"Total : \" + str(total2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e005e7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF : \n",
      "[[ 0.02789294  0.02231436  0.          0.03719059  0.05149467]\n",
      " [-0.         -0.         -0.         -0.         -0.        ]\n",
      " [ 0.12770641  0.          0.12770641  0.0851376   0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.        ]\n",
      " [ 0.11453634  0.09162907  0.          0.          0.        ]\n",
      " [ 0.11453634  0.          0.22907268  0.          0.        ]\n",
      " [ 0.02789294  0.02231436  0.          0.0185953   0.01716489]\n",
      " [ 0.          0.          0.          0.15271512  0.1409678 ]\n",
      " [ 0.          0.20433025  0.          0.0851376   0.11788284]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = np.multiply(w_tf.transpose(),IDF).transpose()\n",
    "print(\"TF-IDF : \\n\" + str(tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e118098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.24719255 0.77248235 0.29760068 0.04759334]\n",
      " [0.24719255 1.         0.         0.41449353 0.59122576]\n",
      " [0.77248235 0.         1.         0.20846468 0.        ]\n",
      " [0.29760068 0.41449353 0.20846468 1.         0.88698707]\n",
      " [0.04759334 0.59122576 0.         0.88698707 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = []\n",
    "tf_idf_t = tf_idf.transpose()\n",
    "for x in tf_idf_t:\n",
    "    for y in tf_idf_t:\n",
    "        cos_sim.append( np.dot(x, y)/(norm(x)*norm(y)) )\n",
    "cos_sim = np.asarray(cos_sim).reshape(5,5)\n",
    "print(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa06908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'really', 'like']\n",
      "['really', 'like', 'python,']\n",
      "['like', 'python,', \"it's\"]\n",
      "['python,', \"it's\", 'pretty']\n",
      "[\"it's\", 'pretty', 'awesome.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I really like python, it's pretty awesome.\".split()\n",
    "N = 3\n",
    "grams = [sentence[i:i+N] for i in range(len(sentence)-N+1)]\n",
    "for gram in grams:\n",
    "    print(gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d2a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3d2ac12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['RT',\n",
       "  '@KirkKus',\n",
       "  ':',\n",
       "  'Indirect',\n",
       "  'cost',\n",
       "  'of',\n",
       "  'the',\n",
       "  'UK',\n",
       "  'being',\n",
       "  'in',\n",
       "  'the',\n",
       "  'EU',\n",
       "  'is',\n",
       "  'estimated',\n",
       "  'to',\n",
       "  'be',\n",
       "  'costing',\n",
       "  'Britain',\n",
       "  '£',\n",
       "  '170',\n",
       "  'billion',\n",
       "  'per',\n",
       "  'year',\n",
       "  '!',\n",
       "  '#BetterOffOut',\n",
       "  '#UKIP'],\n",
       " ['VIDEO',\n",
       "  ':',\n",
       "  'Sturgeon',\n",
       "  'on',\n",
       "  'post-election',\n",
       "  'deals',\n",
       "  'http://t.co/BTJwrpbmOY'],\n",
       " ['RT',\n",
       "  '@LabourEoin',\n",
       "  ':',\n",
       "  'The',\n",
       "  'economy',\n",
       "  'was',\n",
       "  'growing',\n",
       "  '3',\n",
       "  'times',\n",
       "  'faster',\n",
       "  'on',\n",
       "  'the',\n",
       "  'day',\n",
       "  'David',\n",
       "  'Cameron',\n",
       "  'became',\n",
       "  'Prime',\n",
       "  'Minister',\n",
       "  'than',\n",
       "  'it',\n",
       "  'is',\n",
       "  'today',\n",
       "  '..',\n",
       "  '#BBCqt',\n",
       "  'http://t.co…']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
